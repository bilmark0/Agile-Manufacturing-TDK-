{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhpFCqdDwM03PoKoED90rN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bilmark0/Agile-Manufacturing-TDK-/blob/main/CV/CNN_without_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56TKwtITQGS4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Clone the GitHub repository\n",
        "!git clone https://github.com/bilmark0/Agile-Manufacturing-TDK-.git\n",
        "\n",
        "# Append the modules path\n",
        "sys.path.append(\"/content/Agile-Manufacturing-TDK-/CV/modules\")\n",
        "\n",
        "# Import DependencyManager from the correct module path\n",
        "from dependency_manager import DependencyManager\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zYg8phKTQJcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move the uploaded kaggle.json file to the proper directory\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the training data from kaggle\n",
        "!kaggle competitions download -c airbus-ship-detection\n",
        "\n",
        "# Unzip the file and delete the zip afterwards\n",
        "zip_file = '/content/airbus-ship-detection.zip'\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "os.remove(zip_file)\n",
        "if os.path.exists('/content/sample_data'):\n",
        "  shutil.rmtree('/content/sample_data')\n",
        "\n",
        "print(\"Download and extraction complete.\")"
      ],
      "metadata": {
        "id": "1Kho_Ag8QWAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/path/to/your/faulty_directory'\n",
        "img_height, img_width = 240, 380\n",
        "\n",
        "# Data generator with augmentation and validation split\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.4,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.1  # Use 10% of the data for validation\n",
        ")\n",
        "\n",
        "# Training data generator\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'  # Set as training data\n",
        ")\n",
        "\n",
        "# Validation data generator\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'  # Set as validation data\n",
        ")\n",
        "\n",
        "# Build the Custom CNN Model\n",
        "model = Sequential()\n",
        "\n",
        "# Convolutional Block 1\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Convolutional Block 2\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Convolutional Block 3\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Convolutional Block 4\n",
        "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Flatten and Fully Connected Layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))  # Dense layer with 512 neurons\n",
        "model.add(Dropout(0.5))  # Dropout to prevent overfitting\n",
        "model.add(Dense(256, activation='relu'))  # Dense layer with 256 neurons\n",
        "model.add(Dropout(0.5))  # Additional dropout\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Compile the model with a binary cross-entropy loss function for binary classification\n",
        "model.compile(optimizer=Adam(learning_rate=0.00001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=100,  # Increased epochs to take advantage of larger dataset\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(val_generator)\n",
        "print(f'Validation Loss: {loss}, Accuracy: {accuracy}')\n",
        "\n",
        "# Save the trained model\n",
        "model.save('/content/drive/MyDrive/object_detection_custom_cnn_model.h5')"
      ],
      "metadata": {
        "id": "dULA_2FRQZFX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}